{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 00a - Training YOLO11 Dog Pose Model\n\n**FONDAMENTALE**: Questo notebook addestra il backbone YOLO per la detection di **CANI** con **24 keypoints anatomici**.\n\n## Perché questo notebook è necessario\n\nIl modello `yolo11n-pose.pt` standard è addestrato su **COCO Human Pose**:\n- Rileva solo **persone** (classe 0)\n- Estrae **17 keypoints umani** (naso, occhi, spalle, gomiti, polsi, anche, ginocchia, caviglie)\n\nPer ResQPet serve un modello addestrato sul **Dog-Pose Dataset**:\n- Rileva **cani**\n- Estrae **24 keypoints anatomici del cane** (naso, occhi, orecchie, zampe, coda, etc.)\n\n## Dataset\n\nIl [Dog-Pose Dataset](https://docs.ultralytics.com/datasets/pose/dog-pose/) contiene:\n- **6,773 immagini di training**\n- **1,703 immagini di test**\n- **24 keypoints** per cane con coordinate (x, y, visibility)\n\n## Output\n\nI modelli vengono salvati con **versioning automatico** in `ResQPet/weights/`:\n- `yolo11n-dog-pose-v1.pt` - Prima versione\n- `yolo11n-dog-pose-v2.pt` - Seconda versione (150 epochs, mAP@50=98.7%)\n- etc.\n\n## Struttura Path\n\n```\nResQPet/\n├── weights/                    # Modelli condivisi (NON in backend/)\n│   ├── yolo11n-dog-pose-v2.pt  # Backbone attuale\n│   ├── collar_detector.pt\n│   └── ...\n├── datasets/                   # Dataset per test\n│   ├── dog-pose/\n│   ├── stray-dogs-fyp/\n│   └── ...\n└── training/\n    └── notebooks/              # Questo notebook\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione dipendenze\n",
    "%pip install ultralytics torch torchvision onnx onnxruntime -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione paths - RELATIVI per portabilità\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Determina la directory base del progetto (relativa al notebook)\n",
    "# Struttura: ResQPet/training/notebooks/questo_notebook.ipynb\n",
    "NOTEBOOK_DIR = Path.cwd()  # Directory corrente (dove si esegue il notebook)\n",
    "\n",
    "# Se eseguito dalla cartella notebooks\n",
    "if NOTEBOOK_DIR.name == \"notebooks\":\n",
    "    PROJECT_DIR = NOTEBOOK_DIR.parent.parent  # ResQPet\n",
    "elif NOTEBOOK_DIR.name == \"training\":\n",
    "    PROJECT_DIR = NOTEBOOK_DIR.parent  # ResQPet\n",
    "elif NOTEBOOK_DIR.name == \"ResQPet\":\n",
    "    PROJECT_DIR = NOTEBOOK_DIR\n",
    "else:\n",
    "    # Fallback: cerca ResQPet nella struttura\n",
    "    PROJECT_DIR = NOTEBOOK_DIR\n",
    "    while PROJECT_DIR.name != \"ResQPet\" and PROJECT_DIR.parent != PROJECT_DIR:\n",
    "        PROJECT_DIR = PROJECT_DIR.parent\n",
    "    if PROJECT_DIR.name != \"ResQPet\":\n",
    "        PROJECT_DIR = Path.cwd()  # Usa directory corrente come fallback\n",
    "\n",
    "# Directory base che contiene ResQPet e i dataset\n",
    "BASE_DIR = PROJECT_DIR.parent  # Contiene ResQPet, stray-dogs-detection, Stanford Dog, etc.\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"\\nProject directory: {PROJECT_DIR}\")\n",
    "print(f\"Base directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configurazione paths - RELATIVI (CORRETTO)\n# NOTA: weights è nella root del progetto, NON in backend/weights\nWEIGHTS_DIR = PROJECT_DIR / \"weights\"  # Cartella modelli condivisa\nRUNS_DIR = PROJECT_DIR / \"training\" / \"runs\" / \"dog_pose\"\n\nWEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\nRUNS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Determina versione modello (auto-incremento)\nexisting_models = list(WEIGHTS_DIR.glob(\"yolo11n-dog-pose*.pt\"))\nif existing_models:\n    versions = []\n    for m in existing_models:\n        name = m.stem\n        if \"-v\" in name:\n            try:\n                v = int(name.split(\"-v\")[-1])\n                versions.append(v)\n            except:\n                pass\n    MODEL_VERSION = max(versions) + 1 if versions else 2\nelse:\n    MODEL_VERSION = 1\n\nMODEL_NAME = f\"yolo11n-dog-pose-v{MODEL_VERSION}\"\n\nprint(f\"Project: {PROJECT_DIR}\")\nprint(f\"Weights: {WEIGHTS_DIR}\")\nprint(f\"Runs: {RUNS_DIR}\")\nprint(f\"\\nModello output: {MODEL_NAME}.pt (versione {MODEL_VERSION})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione training\n",
    "# Rileva automaticamente il device migliore disponibile\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "CONFIG = {\n",
    "    'model': 'yolo11n-pose.pt',     # Modello base (human pose, verrà fine-tunato)\n",
    "    'data': 'dog-pose.yaml',         # Dataset config (auto-download da Ultralytics)\n",
    "    'epochs': 150,                   # Numero epoche\n",
    "    'imgsz': 640,                    # Dimensione immagine\n",
    "    'batch': 16,                     # Batch size (ridurre se OOM)\n",
    "    'patience': 20,                  # Early stopping patience\n",
    "    'device': DEVICE,                # Auto-detect: 'cuda', 'mps', o 'cpu'\n",
    "    'workers': 4,                    # Data loader workers\n",
    "    'optimizer': 'AdamW',            # Optimizer\n",
    "    'lr0': 0.001,                    # Learning rate iniziale\n",
    "    'lrf': 0.01,                     # Learning rate finale (fraction)\n",
    "    'mosaic': 1.0,                   # Mosaic augmentation\n",
    "    'mixup': 0.1,                    # Mixup augmentation\n",
    "    'degrees': 10.0,                 # Rotation augmentation\n",
    "    'translate': 0.1,                # Translation augmentation\n",
    "    'scale': 0.5,                    # Scale augmentation\n",
    "    'fliplr': 0.5,                   # Horizontal flip probability\n",
    "}\n",
    "\n",
    "print(\"Configurazione Training:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Caricamento Modello Base\n",
    "\n",
    "Partiamo da `yolo11n-pose.pt` (human pose) e lo fine-tuniamo sul Dog-Pose dataset.\n",
    "Questo approccio di **transfer learning** è più efficiente che partire da zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CARICAMENTO MODELLO BASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Carica modello base (scarica automaticamente se non presente)\n",
    "model = YOLO(CONFIG['model'])\n",
    "\n",
    "print(f\"\\nModello caricato: {CONFIG['model']}\")\n",
    "print(f\"Task: {model.task}\")\n",
    "print(f\"\\nInfo modello:\")\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verifica Dataset Dog-Pose\n",
    "\n",
    "Il dataset verrà scaricato automaticamente da Ultralytics quando avviamo il training.\n",
    "Verifichiamo prima la configurazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra configurazione dataset dog-pose\n",
    "print(\"=\"*60)\n",
    "print(\"DOG-POSE DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Il Dog-Pose Dataset contiene:\n",
    "\n",
    "- Training: 6,773 immagini\n",
    "- Test: 1,703 immagini\n",
    "- Classi: 1 (Dog)\n",
    "- Keypoints: 24\n",
    "\n",
    "Keypoints anatomici del cane:\n",
    "  0: nose\n",
    "  1: left_eye\n",
    "  2: right_eye  \n",
    "  3: left_ear_base\n",
    "  4: right_ear_base\n",
    "  5: left_ear_tip\n",
    "  6: right_ear_tip\n",
    "  7: throat\n",
    "  8: withers (garrese)\n",
    "  9: left_front_elbow\n",
    " 10: right_front_elbow\n",
    " 11: left_front_knee\n",
    " 12: right_front_knee\n",
    " 13: left_front_paw\n",
    " 14: right_front_paw\n",
    " 15: left_back_elbow\n",
    " 16: right_back_elbow\n",
    " 17: left_back_knee\n",
    " 18: right_back_knee\n",
    " 19: left_back_paw\n",
    " 20: right_back_paw\n",
    " 21: tail_start\n",
    " 22: tail_end\n",
    " 23: chin\n",
    "\n",
    "URL: https://docs.ultralytics.com/datasets/pose/dog-pose/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Avviamo il training. Il dataset verrà scaricato automaticamente.\n",
    "\n",
    "**Tempo stimato:**\n",
    "- Mac M1/M2: ~2-3 ore\n",
    "- NVIDIA GPU: ~1-2 ore\n",
    "- CPU: ~8-12 ore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"AVVIO TRAINING YOLO DOG-POSE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(\"\\nIl dataset Dog-Pose verrà scaricato automaticamente (~500MB)...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Avvia training\n",
    "results = model.train(\n",
    "    data=CONFIG['data'],\n",
    "    epochs=CONFIG['epochs'],\n",
    "    imgsz=CONFIG['imgsz'],\n",
    "    batch=CONFIG['batch'],\n",
    "    patience=CONFIG['patience'],\n",
    "    device=CONFIG['device'],\n",
    "    workers=CONFIG['workers'],\n",
    "    optimizer=CONFIG['optimizer'],\n",
    "    lr0=CONFIG['lr0'],\n",
    "    lrf=CONFIG['lrf'],\n",
    "    mosaic=CONFIG['mosaic'],\n",
    "    mixup=CONFIG['mixup'],\n",
    "    degrees=CONFIG['degrees'],\n",
    "    translate=CONFIG['translate'],\n",
    "    scale=CONFIG['scale'],\n",
    "    fliplr=CONFIG['fliplr'],\n",
    "    project=str(RUNS_DIR),\n",
    "    name='yolo11n_dog_pose',\n",
    "    exist_ok=True,\n",
    "    verbose=True,\n",
    "    plots=True,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETATO!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trova il best model\n",
    "best_model_path = RUNS_DIR / 'yolo11n_dog_pose' / 'weights' / 'best.pt'\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(f\"Best model trovato: {best_model_path}\")\n",
    "    \n",
    "    # Carica best model\n",
    "    best_model = YOLO(str(best_model_path))\n",
    "    \n",
    "    # Valutazione su validation set\n",
    "    print(\"\\nValutazione su validation set...\")\n",
    "    metrics = best_model.val()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"METRICHE VALUTAZIONE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "    print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "    if hasattr(metrics, 'pose'):\n",
    "        print(f\"Pose mAP50: {metrics.pose.map50:.4f}\")\n",
    "        print(f\"Pose mAP50-95: {metrics.pose.map:.4f}\")\n",
    "else:\n",
    "    print(f\"Best model non trovato: {best_model_path}\")\n",
    "    print(\"Esegui prima il training nella cella precedente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test su Immagini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\n\n# Trova immagini di test dai dataset INTERNI al progetto\nDATASETS_DIR = PROJECT_DIR / \"datasets\"\n\ntest_dirs = [\n    DATASETS_DIR / \"stray-dogs-fyp\" / \"images\" / \"testing\",\n    DATASETS_DIR / \"dog-pose\" / \"test\" / \"images\",\n    DATASETS_DIR / \"stanford-dogs\" / \"Images\",\n    DATASETS_DIR / \"dog-with-leash\" / \"images\",\n]\n\ntest_images = []\nfor test_dir in test_dirs:\n    if test_dir.exists():\n        # Cerca jpg e png\n        images = list(test_dir.glob('**/*.jpg'))[:3]\n        images += list(test_dir.glob('**/*.png'))[:3]\n        test_images.extend(images[:3])\n        print(f\"Trovate immagini in {test_dir.relative_to(PROJECT_DIR)}\")\n\nif not test_images:\n    print(\"\\nNessun dataset trovato. Struttura attesa:\")\n    print(\"  ResQPet/datasets/stray-dogs-fyp/images/testing/\")\n    print(\"  ResQPet/datasets/dog-pose/test/images/\")\n    print(\"  ResQPet/datasets/stanford-dogs/Images/\")\n\nprint(f\"\\nTotale immagini test: {len(test_images)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_path.exists() and test_images:\n",
    "    best_model = YOLO(str(best_model_path))\n",
    "    \n",
    "    # Test su alcune immagini\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, img_path in enumerate(test_images[:6]):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Inference\n",
    "        results = best_model(str(img_path), verbose=False)\n",
    "        \n",
    "        # Plot risultato\n",
    "        result_img = results[0].plot()\n",
    "        axes[i].imshow(result_img[..., ::-1])  # BGR to RGB\n",
    "        \n",
    "        # Info\n",
    "        n_dogs = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "        axes[i].set_title(f\"{img_path.name}\\n{n_dogs} dog(s) detected\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Nascondi assi vuoti\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RUNS_DIR / 'test_results.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Esegui prima il training o aggiungi immagini di test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica keypoints estratti\n",
    "if best_model_path.exists() and test_images:\n",
    "    best_model = YOLO(str(best_model_path))\n",
    "    \n",
    "    # Test su prima immagine\n",
    "    test_img = str(test_images[0])\n",
    "    results = best_model(test_img, verbose=False)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"VERIFICA KEYPOINTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nImmagine: {test_img}\")\n",
    "    \n",
    "    if results[0].keypoints is not None:\n",
    "        kpts = results[0].keypoints.data[0].cpu().numpy()\n",
    "        print(f\"\\nKeypoints shape: {kpts.shape}\")\n",
    "        print(f\"Numero keypoints: {kpts.shape[0]}\")\n",
    "        print(f\"\\nPrimi 5 keypoints (x, y, confidence):\")\n",
    "        \n",
    "        keypoint_names = [\n",
    "            'nose', 'left_eye', 'right_eye', 'left_ear_base', 'right_ear_base',\n",
    "            'left_ear_tip', 'right_ear_tip', 'throat', 'withers',\n",
    "            'left_front_elbow', 'right_front_elbow', 'left_front_knee', 'right_front_knee',\n",
    "            'left_front_paw', 'right_front_paw', 'left_back_elbow', 'right_back_elbow',\n",
    "            'left_back_knee', 'right_back_knee', 'left_back_paw', 'right_back_paw',\n",
    "            'tail_start', 'tail_end', 'chin'\n",
    "        ]\n",
    "        \n",
    "        for i, kpt in enumerate(kpts[:5]):\n",
    "            name = keypoint_names[i] if i < len(keypoint_names) else f'kpt_{i}'\n",
    "            print(f\"  {i:2d}. {name:20s}: x={kpt[0]:.1f}, y={kpt[1]:.1f}, conf={kpt[2]:.2f}\")\n",
    "        \n",
    "        print(f\"\\n... e altri {kpts.shape[0] - 5} keypoints\")\n",
    "    else:\n",
    "        print(\"\\nNessun keypoint rilevato (nessun cane nell'immagine?)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Modello\n",
    "\n",
    "Esportiamo il modello in:\n",
    "1. **PyTorch** (.pt) - Per uso nel backend Python\n",
    "2. **ONNX** (.onnx) - Per deploy cross-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"EXPORT MODELLO\")\nprint(\"=\"*60)\n\nif best_model_path.exists():\n    best_model = YOLO(str(best_model_path))\n    \n    # 1. Copia PyTorch model nella cartella weights con versioning\n    pt_output = WEIGHTS_DIR / f'{MODEL_NAME}.pt'\n    shutil.copy(best_model_path, pt_output)\n    print(f\"\\n[1/2] PyTorch model salvato: {pt_output}\")\n    print(f\"      Dimensione: {pt_output.stat().st_size / 1024 / 1024:.2f} MB\")\n    \n    # 2. Export ONNX\n    print(\"\\n[2/2] Esportazione ONNX...\")\n    onnx_output = best_model.export(\n        format='onnx',\n        imgsz=640,\n        simplify=True,\n        dynamic=False,\n        opset=12\n    )\n    \n    # Sposta ONNX nella cartella weights con stesso nome\n    onnx_source = Path(onnx_output)\n    onnx_dest = WEIGHTS_DIR / f'{MODEL_NAME}.onnx'\n    if onnx_source.exists():\n        shutil.move(str(onnx_source), str(onnx_dest))\n        print(f\"      ONNX model salvato: {onnx_dest}\")\n        print(f\"      Dimensione: {onnx_dest.stat().st_size / 1024 / 1024:.2f} MB\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"EXPORT COMPLETATO!\")\n    print(\"=\"*60)\n    print(f\"\\nPer usare questo modello, aggiorna i config:\")\n    print(f\"  BACKBONE_MODEL = WEIGHTS_DIR / '{MODEL_NAME}.pt'\")\nelse:\n    print(\"Best model non trovato. Esegui prima il training.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Modello Esportato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test modello PyTorch esportato\npt_model_path = WEIGHTS_DIR / f'{MODEL_NAME}.pt'\n\nif pt_model_path.exists() and test_images:\n    print(f\"Test modello PyTorch esportato: {MODEL_NAME}.pt\")\n    \n    exported_model = YOLO(str(pt_model_path))\n    \n    # Test\n    results = exported_model(str(test_images[0]), verbose=False)\n    \n    print(f\"\\nRisultato:\")\n    print(f\"  Boxes: {len(results[0].boxes) if results[0].boxes is not None else 0}\")\n    if results[0].keypoints is not None:\n        print(f\"  Keypoints shape: {results[0].keypoints.data.shape}\")\n        print(f\"  Numero keypoints per detection: {results[0].keypoints.data.shape[1]}\")\n    \n    print(\"\\n Modello PyTorch funziona correttamente!\")\nelse:\n    print(\"Modello non trovato o nessuna immagine di test.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test modello ONNX\nonnx_model_path = WEIGHTS_DIR / f'{MODEL_NAME}.onnx'\n\nif onnx_model_path.exists() and test_images:\n    print(f\"Test modello ONNX esportato: {MODEL_NAME}.onnx\")\n    \n    # YOLO può caricare direttamente ONNX\n    onnx_model = YOLO(str(onnx_model_path))\n    \n    # Test\n    results = onnx_model(str(test_images[0]), verbose=False)\n    \n    print(f\"\\nRisultato:\")\n    print(f\"  Boxes: {len(results[0].boxes) if results[0].boxes is not None else 0}\")\n    if results[0].keypoints is not None:\n        print(f\"  Keypoints shape: {results[0].keypoints.data.shape}\")\n    \n    print(\"\\n Modello ONNX funziona correttamente!\")\nelse:\n    print(\"Modello ONNX non trovato o nessuna immagine di test.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Aggiornamento Configurazione Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"AGGIORNAMENTO CONFIGURAZIONE\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nPer usare il nuovo modello {MODEL_NAME}, aggiorna:\n\n1. backend/app/config.py:\n   \n   MODELS = {{\n       'backbone': WEIGHTS_DIR / '{MODEL_NAME}.pt',  # <-- AGGIORNATO\n       'collar': WEIGHTS_DIR / 'collar_detector.pt',\n       'skin': WEIGHTS_DIR / 'skin_classifier.pt',\n       'pose': WEIGHTS_DIR / 'stray_pose_classifier.pt',\n       'breed': WEIGHTS_DIR / 'breed_classifier.pt',\n   }}\n\n2. labeling_tool/config.py:\n   \n   BACKBONE_MODEL = WEIGHTS_DIR / '{MODEL_NAME}.pt'\n\n3. Ri-esegui notebook 00_keypoints_extraction.ipynb\n   per estrarre i 24 keypoints corretti dai dataset.\n\n4. Ri-esegui notebook 03_pose_classifier.ipynb\n   con input_dim = 72 (24 keypoints × 3).\n\n5. Riavvia il backend:\n   cd backend && python run.py\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Riepilogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"RIEPILOGO TRAINING YOLO DOG-POSE\")\nprint(\"=\"*60)\n\nprint(f\"\"\"\nDataset: Dog-Pose (Ultralytics)\n  - Training: 6,773 immagini\n  - Test: 1,703 immagini\n  - Keypoints: 24 anatomici del cane\n\nModello Base: yolo11n-pose.pt (human pose)\n  - Fine-tuned su dog-pose dataset\n  - Transfer learning\n\nTraining:\n  - Epochs: {CONFIG['epochs']}\n  - Image Size: {CONFIG['imgsz']}\n  - Batch Size: {CONFIG['batch']}\n  - Device: {CONFIG['device']}\n\nOutput (versione {MODEL_VERSION}):\n  - PyTorch: {WEIGHTS_DIR / f'{MODEL_NAME}.pt'}\n  - ONNX: {WEIGHTS_DIR / f'{MODEL_NAME}.onnx'}\n\nKeypoints del cane (24):\n  - Testa: nose, eyes, ears, chin, throat\n  - Corpo: withers (garrese)\n  - Zampe anteriori: elbow, knee, paw (L/R)\n  - Zampe posteriori: elbow, knee, paw (L/R)  \n  - Coda: tail_start, tail_end\n\nProssimi passi:\n  1. Aggiorna config.py per usare {MODEL_NAME}.pt\n  2. Ri-esegui notebook 00 per estrarre keypoints\n  3. Ri-esegui notebook 03 per addestrare pose classifier\n  4. Riavvia backend\n\"\"\")\n\n# Verifica file creati\nprint(\"\\nFile nella cartella weights:\")\nfor f in sorted(WEIGHTS_DIR.glob(\"yolo11n-dog-pose*.pt\")):\n    size = f.stat().st_size / 1024 / 1024\n    print(f\"   {f.name}: {size:.2f} MB\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}