{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Pipeline Demo ResQPet\n",
    "\n",
    "Dimostrazione della pipeline completa per la detection di cani randagi.\n",
    "\n",
    "## Pipeline\n",
    "```\n",
    "Input Image\n",
    "    â”‚\n",
    "    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   YOLO11 Pose     â”‚ â†’ Keypoints (24 punti)\n",
    "â”‚   (Backbone)      â”‚ â†’ Bounding Box\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼                  â–¼                  â–¼                  â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Collar  â”‚      â”‚  Skin   â”‚      â”‚  Pose   â”‚      â”‚ Breed   â”‚\n",
    "â”‚Detector â”‚      â”‚Classifierâ”‚     â”‚Classifierâ”‚     â”‚Classifierâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚                  â”‚                  â”‚                  â”‚\n",
    "    â–¼                  â–¼                  â–¼                  â–¼\n",
    "P(no_collar)      P(disease)       P(stray_pose)    P(stray|breed)\n",
    "    â”‚                  â”‚                  â”‚                  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Fusion Module  â”‚\n",
    "                    â”‚   (Weighted)    â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                      STRAY INDEX [0,1]\n",
    "```\n",
    "\n",
    "## Weights\n",
    "- Collar: 0.35 (indicatore forte)\n",
    "- Skin: 0.20 (condizioni sanitarie)\n",
    "- Pose: 0.25 (comportamento)\n",
    "- Breed: 0.20 (prior statistico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione dipendenze\n",
    "%pip install ultralytics torch torchvision timm pillow matplotlib opencv-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configurazione paths (usando notebook_utils per portabilitÃ )\nimport sys\nfrom pathlib import Path\n\n# Aggiungi la directory notebooks al path per importare notebook_utils\nnotebooks_dir = Path.cwd()\nif notebooks_dir.name != \"notebooks\":\n    notebooks_dir = Path(__file__).parent if \"__file__\" in dir() else Path.cwd()\nif str(notebooks_dir) not in sys.path:\n    sys.path.insert(0, str(notebooks_dir))\n\nfrom notebook_utils import get_paths, get_device, print_paths\n\n# Ottieni tutti i path in modo relativo\npaths = get_paths()\nprint_paths(paths)\n\n# Estrai path principali\nPROJECT_DIR = paths['project_dir']\nBASE_DIR = paths['base_dir']\nWEIGHTS_DIR = paths['weights_dir']\nDATA_DIR = paths['data_dir']\n\n# Device auto-detection\nDEVICE, DEVICE_NAME = get_device()\nprint(f\"\\nğŸ–¥ï¸ Device: {DEVICE} ({DEVICE_NAME})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configurazione Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass PipelineConfig:\n    \"\"\"Configurazione pipeline ResQPet\"\"\"\n    # Modelli\n    # IMPORTANTE: usa yolo11n-dog-pose.pt (non yolo11n-pose.pt che rileva persone!)\n    backbone_model: str = \"yolo11n-dog-pose.pt\"  # Dog detection with 24 keypoints\n    collar_model: str = \"collar_detector.pt\"\n    skin_model: str = \"skin_classifier.pt\"\n    pose_model: str = \"stray_pose_classifier.pt\"\n    breed_model: str = \"breed_classifier.pt\"\n    \n    # Pesi fusion\n    weight_collar: float = 0.35\n    weight_skin: float = 0.20\n    weight_pose: float = 0.25\n    weight_breed: float = 0.20\n    \n    # Soglie\n    stray_threshold: float = 0.6\n    confidence_threshold: float = 0.5\n    \n    # Image processing\n    img_size: int = 640\n    classifier_img_size: int = 224\n\nconfig = PipelineConfig()\nprint(\"Configurazione pipeline:\")\nfor key, value in config.__dict__.items():\n    print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DogDetection:\n",
    "    \"\"\"Risultato detection per un singolo cane\"\"\"\n",
    "    # Bounding box\n",
    "    bbox: Tuple[int, int, int, int]  # x1, y1, x2, y2\n",
    "    confidence: float\n",
    "    \n",
    "    # Keypoints\n",
    "    keypoints: Optional[np.ndarray] = None\n",
    "    \n",
    "    # ProbabilitÃ  individuali\n",
    "    p_no_collar: float = 0.5\n",
    "    p_disease: float = 0.5\n",
    "    p_stray_pose: float = 0.5\n",
    "    p_stray_breed: float = 0.5\n",
    "    \n",
    "    # Breed info\n",
    "    predicted_breed: str = \"unknown\"\n",
    "    breed_confidence: float = 0.0\n",
    "    \n",
    "    # Stray Index finale\n",
    "    stray_index: float = 0.5\n",
    "    is_stray: bool = False\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'bbox': self.bbox,\n",
    "            'confidence': self.confidence,\n",
    "            'stray_index': self.stray_index,\n",
    "            'is_stray': self.is_stray,\n",
    "            'components': {\n",
    "                'p_no_collar': self.p_no_collar,\n",
    "                'p_disease': self.p_disease,\n",
    "                'p_stray_pose': self.p_stray_pose,\n",
    "                'p_stray_breed': self.p_stray_breed\n",
    "            },\n",
    "            'breed': {\n",
    "                'predicted': self.predicted_breed,\n",
    "                'confidence': self.breed_confidence\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class StrayPoseMLP(nn.Module):\n    \"\"\"MLP per classificazione pose (copia dal notebook 03)\"\"\"\n    def __init__(self, input_dim=72, hidden_dims=[128, 64], dropout=0.3):\n        \"\"\"\n        Args:\n            input_dim: 72 per Dog-Pose (24 keypoints Ã— 3), 51 per COCO (17 keypoints Ã— 3)\n        \"\"\"\n        super().__init__()\n        layers = []\n        prev_dim = input_dim\n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n                nn.BatchNorm1d(hidden_dim),  # BatchNorm PRIMA di Dropout\n                nn.Dropout(dropout)\n            ])\n            prev_dim = hidden_dim\n        layers.append(nn.Linear(prev_dim, 1))\n        layers.append(nn.Sigmoid())\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x).squeeze(-1)\n\n\nclass BreedClassifier(nn.Module):\n    \"\"\"EfficientNet per classificazione razze (copia dal notebook 04)\"\"\"\n    def __init__(self, num_classes, pretrained=False):\n        super().__init__()\n        self.backbone = timm.create_model('efficientnet_b0', pretrained=pretrained, num_classes=0)\n        self.feature_dim = self.backbone.num_features\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(self.feature_dim, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n        self.num_classes = num_classes\n    \n    def forward(self, x):\n        features = self.backbone(x)\n        return self.classifier(features)\n    \n    def predict_proba(self, x):\n        return torch.softmax(self.forward(x), dim=-1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ResQPetPipeline:\n    \"\"\"Pipeline completa per detection cani randagi\"\"\"\n    \n    def __init__(self, config: PipelineConfig, weights_dir: Path, device: torch.device):\n        self.config = config\n        self.weights_dir = weights_dir\n        self.device = device\n        self.pose_input_dim = 72  # Default: 24 keypoints Ã— 3 (Dog-Pose)\n        \n        # Carica modelli\n        self._load_models()\n        \n        # Trasformazioni per classificatori\n        self.classifier_transform = transforms.Compose([\n            transforms.Resize((config.classifier_img_size, config.classifier_img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Carica breed priors\n        priors_file = weights_dir.parent.parent / 'data' / 'breed_priors.json'\n        if priors_file.exists():\n            with open(priors_file) as f:\n                self.breed_priors = json.load(f)\n        else:\n            self.breed_priors = {'unknown': 0.5}\n    \n    def _load_models(self):\n        \"\"\"Carica tutti i modelli\"\"\"\n        print(\"Caricamento modelli...\")\n        \n        # 1. Backbone (YOLO11 Pose) - scarica se non esiste\n        backbone_path = self.weights_dir / self.config.backbone_model\n        if backbone_path.exists():\n            self.backbone = YOLO(str(backbone_path))\n            self.use_pose_model = True\n            print(f\"  âœ“ Backbone: {backbone_path.name}\")\n        else:\n            # Scarica yolo11n-pose automaticamente da ultralytics\n            print(f\"  â³ Scaricamento yolo11n-pose.pt...\")\n            try:\n                self.backbone = YOLO('yolo11n-pose.pt')  # Ultralytics scarica automaticamente\n                self.use_pose_model = True\n                print(f\"  âœ“ Backbone: yolo11n-pose.pt (scaricato)\")\n            except Exception as e:\n                print(f\"  âš  Impossibile scaricare yolo11n-pose: {e}\")\n                print(f\"  âš  Usando yolov8n.pt come fallback\")\n                self.backbone = YOLO('yolov8n.pt')\n                self.use_pose_model = False\n        \n        # 2. Collar Detector\n        collar_path = self.weights_dir / self.config.collar_model\n        if collar_path.exists():\n            self.collar_detector = YOLO(str(collar_path))\n            print(f\"  âœ“ Collar Detector: {collar_path.name}\")\n        else:\n            self.collar_detector = None\n            print(f\"  âš  Collar Detector: usando euristica\")\n        \n        # 3. Skin Classifier (weights_only=False per PyTorch 2.6+)\n        skin_path = self.weights_dir / self.config.skin_model\n        if skin_path.exists():\n            checkpoint = torch.load(skin_path, map_location=self.device, weights_only=False)\n            self.skin_classifier = timm.create_model('resnet50', pretrained=False, num_classes=checkpoint.get('num_classes', 6))\n            self.skin_classifier.load_state_dict(checkpoint['model_state_dict'])\n            self.skin_classifier.to(self.device)\n            self.skin_classifier.eval()\n            print(f\"  âœ“ Skin Classifier: {skin_path.name}\")\n        else:\n            self.skin_classifier = None\n            print(f\"  âš  Skin Classifier: usando euristica\")\n        \n        # 4. Pose Classifier (weights_only=False per PyTorch 2.6+)\n        pose_path = self.weights_dir / self.config.pose_model\n        if pose_path.exists():\n            checkpoint = torch.load(pose_path, map_location=self.device, weights_only=False)\n            # IMPORTANTE: leggi input_dim dal checkpoint per compatibilitÃ \n            self.pose_input_dim = checkpoint.get('input_dim', 72)  # 72 per Dog-Pose, 51 per COCO\n            hidden_dims = checkpoint.get('hidden_dims', [128, 64])\n            self.pose_classifier = StrayPoseMLP(input_dim=self.pose_input_dim, hidden_dims=hidden_dims)\n            self.pose_classifier.load_state_dict(checkpoint['model_state_dict'])\n            self.pose_classifier.to(self.device)\n            self.pose_classifier.eval()\n            print(f\"  âœ“ Pose Classifier: {pose_path.name} (input_dim={self.pose_input_dim})\")\n        else:\n            self.pose_classifier = None\n            print(f\"  âš  Pose Classifier: usando euristica\")\n        \n        # 5. Breed Classifier (weights_only=False per PyTorch 2.6+)\n        breed_path = self.weights_dir / self.config.breed_model\n        if breed_path.exists():\n            checkpoint = torch.load(breed_path, map_location=self.device, weights_only=False)\n            num_classes = len(checkpoint.get('categories', ['unknown']))\n            self.breed_classifier = BreedClassifier(num_classes=num_classes)\n            self.breed_classifier.load_state_dict(checkpoint['model_state_dict'])\n            self.breed_classifier.to(self.device)\n            self.breed_classifier.eval()\n            self.breed_categories = checkpoint.get('categories', ['unknown'])\n            print(f\"  âœ“ Breed Classifier: {breed_path.name}\")\n        else:\n            self.breed_classifier = None\n            self.breed_categories = ['unknown']\n            print(f\"  âš  Breed Classifier: usando euristica\")\n        \n        print(\"Modelli caricati!\")\n    \n    def preprocess_keypoints(self, keypoints: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Preprocessa keypoints per il classificatore pose.\n        \n        Gestisce dinamicamente:\n        - 24 keypoints (Dog-Pose) â†’ 72 features\n        - 17 keypoints (COCO Human) â†’ 51 features\n        \n        L'output deve matchare self.pose_input_dim del modello caricato.\n        \"\"\"\n        num_keypoints = len(keypoints)\n        target_dim = self.pose_input_dim  # 72 o 51 dal modello\n        \n        # Filtra keypoints con bassa confidence\n        valid_kpts = keypoints[keypoints[:, 2] > 0.3]\n        if len(valid_kpts) < 3:\n            return np.zeros(target_dim)\n        \n        # Normalizza rispetto al centro e scala\n        center_x = valid_kpts[:, 0].mean()\n        center_y = valid_kpts[:, 1].mean()\n        scale = max(valid_kpts[:, 0].max() - valid_kpts[:, 0].min(),\n                   valid_kpts[:, 1].max() - valid_kpts[:, 1].min()) + 1e-6\n        \n        normalized = keypoints.copy()\n        normalized[:, 0] = (keypoints[:, 0] - center_x) / scale\n        normalized[:, 1] = (keypoints[:, 1] - center_y) / scale\n        # La confidence (colonna 2) rimane invariata\n        \n        # Flatten e adatta alla dimensione target\n        features = normalized.flatten()\n        \n        if len(features) < target_dim:\n            # Pad con zeri se ci sono meno keypoints del previsto\n            features = np.pad(features, (0, target_dim - len(features)))\n        elif len(features) > target_dim:\n            # Trunca se ci sono piÃ¹ keypoints del previsto\n            features = features[:target_dim]\n        \n        return features\n    \n    def _predict_collar(self, image: Image.Image) -> float:\n        \"\"\"Predice P(no_collar)\"\"\"\n        if self.collar_detector is None:\n            return 0.5\n        \n        results = self.collar_detector(image, verbose=False)\n        \n        for result in results:\n            for box in result.boxes:\n                cls = int(box.cls[0])\n                conf = float(box.conf[0])\n                if cls == 0:  # Con guinzaglio\n                    return 1.0 - conf\n                elif cls == 1:  # Senza guinzaglio\n                    return conf\n        \n        return 0.7  # Default: probabilmente senza\n    \n    def _predict_skin(self, image: Image.Image) -> float:\n        \"\"\"Predice P(disease)\"\"\"\n        if self.skin_classifier is None:\n            return 0.3\n        \n        img_tensor = self.classifier_transform(image).unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n            output = self.skin_classifier(img_tensor)\n            probs = torch.softmax(output, dim=-1)\n            # Somma probabilitÃ  di tutte le classi di malattia (non Healthy)\n            # Assumendo che classe 0 sia Healthy\n            p_disease = 1.0 - probs[0, 0].item()\n            return p_disease\n    \n    def _predict_pose(self, keypoints: np.ndarray) -> float:\n        \"\"\"Predice P(stray_pose)\"\"\"\n        if self.pose_classifier is None or keypoints is None:\n            return 0.5\n        \n        features = self.preprocess_keypoints(keypoints)\n        features_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n            output = self.pose_classifier(features_tensor)\n            return output.item()\n    \n    def _predict_breed(self, image: Image.Image) -> Tuple[str, float, float]:\n        \"\"\"Predice razza e ritorna P(stray|breed)\"\"\"\n        if self.breed_classifier is None:\n            return 'unknown', 0.0, 0.5\n        \n        img_tensor = self.classifier_transform(image).unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n            probs = self.breed_classifier.predict_proba(img_tensor)\n            top_prob, top_idx = probs.max(dim=1)\n            \n            breed = self.breed_categories[top_idx.item()]\n            confidence = top_prob.item()\n            stray_prior = self.breed_priors.get(breed, 0.5)\n            \n            return breed, confidence, stray_prior\n    \n    def compute_stray_index(self, p_collar: float, p_skin: float, \n                           p_pose: float, p_breed: float) -> float:\n        \"\"\"Calcola Stray Index pesato\"\"\"\n        weights = [\n            self.config.weight_collar,\n            self.config.weight_skin,\n            self.config.weight_pose,\n            self.config.weight_breed\n        ]\n        probabilities = [p_collar, p_skin, p_pose, p_breed]\n        stray_index = sum(w * p for w, p in zip(weights, probabilities))\n        return np.clip(stray_index, 0, 1)\n    \n    def process_image(self, image_path: str) -> List[DogDetection]:\n        \"\"\"Processa un'immagine e ritorna le detection\"\"\"\n        image = Image.open(image_path).convert('RGB')\n        \n        # Detection con backbone\n        results = self.backbone(image, verbose=False)\n        \n        detections = []\n        \n        for result in results:\n            for i, box in enumerate(result.boxes):\n                cls = int(box.cls[0])\n                \n                # Per modelli pose: classe 0 = persona, per COCO: classe 16 = cane\n                # YOLO11-pose Ã¨ trainato su persone, quindi accettiamo tutte le detection\n                # e usiamo il collar_detector per filtrare\n                if not self.use_pose_model:\n                    # Per yolov8n generico, filtra solo cani (classe 16 COCO)\n                    if cls != 16:\n                        continue\n                \n                conf = float(box.conf[0])\n                if conf < self.config.confidence_threshold:\n                    continue\n                \n                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n                \n                # Keypoints (se disponibili)\n                keypoints = None\n                if hasattr(result, 'keypoints') and result.keypoints is not None:\n                    if i < len(result.keypoints):\n                        keypoints = result.keypoints[i].data[0].cpu().numpy()\n                \n                # Crop immagine per classificatori\n                crop = image.crop((x1, y1, x2, y2))\n                \n                # Predizioni individuali\n                p_collar = self._predict_collar(crop)\n                p_skin = self._predict_skin(crop)\n                p_pose = self._predict_pose(keypoints)\n                breed, breed_conf, p_breed = self._predict_breed(crop)\n                \n                # Stray Index\n                stray_index = self.compute_stray_index(p_collar, p_skin, p_pose, p_breed)\n                is_stray = stray_index >= self.config.stray_threshold\n                \n                detection = DogDetection(\n                    bbox=(x1, y1, x2, y2),\n                    confidence=conf,\n                    keypoints=keypoints,\n                    p_no_collar=p_collar,\n                    p_disease=p_skin,\n                    p_stray_pose=p_pose,\n                    p_stray_breed=p_breed,\n                    predicted_breed=breed,\n                    breed_confidence=breed_conf,\n                    stray_index=stray_index,\n                    is_stray=is_stray\n                )\n                detections.append(detection)\n        \n        return detections"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza pipeline\n",
    "pipeline = ResQPetPipeline(config, WEIGHTS_DIR, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demo su Immagini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection(image_path: str, detections: List[DogDetection], \n",
    "                        save_path: Optional[str] = None):\n",
    "    \"\"\"Visualizza le detection su un'immagine\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Immagine con bounding box\n",
    "    axes[0].imshow(image)\n",
    "    \n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det.bbox\n",
    "        \n",
    "        # Colore basato su stray_index\n",
    "        if det.is_stray:\n",
    "            color = 'red'\n",
    "            label = f\"STRAY ({det.stray_index:.0%})\"\n",
    "        else:\n",
    "            color = 'green'\n",
    "            label = f\"OWNED ({det.stray_index:.0%})\"\n",
    "        \n",
    "        # Bounding box\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                  linewidth=3, edgecolor=color, facecolor='none')\n",
    "        axes[0].add_patch(rect)\n",
    "        \n",
    "        # Label\n",
    "        axes[0].text(x1, y1-10, label, color=color, fontsize=12, \n",
    "                    fontweight='bold', backgroundcolor='white')\n",
    "        \n",
    "        # Keypoints\n",
    "        if det.keypoints is not None:\n",
    "            for kpt in det.keypoints:\n",
    "                if kpt[2] > 0.3:  # Confidence > 0.3\n",
    "                    axes[0].scatter(kpt[0], kpt[1], c='yellow', s=30, zorder=5)\n",
    "    \n",
    "    axes[0].set_title('Detection Results')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Breakdown probabilitÃ \n",
    "    if detections:\n",
    "        det = detections[0]  # Prima detection\n",
    "        \n",
    "        components = ['No Collar', 'Disease', 'Stray Pose', f'Breed ({det.predicted_breed})']\n",
    "        probs = [det.p_no_collar, det.p_disease, det.p_stray_pose, det.p_stray_breed]\n",
    "        weights = [config.weight_collar, config.weight_skin, \n",
    "                  config.weight_pose, config.weight_breed]\n",
    "        \n",
    "        # Bar chart\n",
    "        y_pos = np.arange(len(components))\n",
    "        colors = plt.cm.RdYlGn_r(probs)\n",
    "        \n",
    "        bars = axes[1].barh(y_pos, probs, color=colors)\n",
    "        axes[1].set_yticks(y_pos)\n",
    "        axes[1].set_yticklabels([f\"{c}\\n(w={w})\" for c, w in zip(components, weights)])\n",
    "        axes[1].set_xlim(0, 1)\n",
    "        axes[1].set_xlabel('Probability')\n",
    "        axes[1].set_title(f'Stray Index Breakdown\\nFinal: {det.stray_index:.2%}')\n",
    "        \n",
    "        # Aggiungi valori sulle barre\n",
    "        for bar, prob in zip(bars, probs):\n",
    "            axes[1].text(prob + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "                        f'{prob:.2f}', va='center')\n",
    "        \n",
    "        # Linea threshold\n",
    "        axes[1].axvline(x=config.stray_threshold, color='red', \n",
    "                       linestyle='--', label=f'Threshold ({config.stray_threshold})')\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No dogs detected', ha='center', va='center')\n",
    "        axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Trova immagini di test usando i path relativi\ntest_dirs = [\n    paths['fyp_dataset'] / \"images\" / \"testing\",      # FYP stray dogs\n    paths['collar_dataset'] / \"split_dataset\" / \"test\" / \"images\",  # Collar dataset\n    paths['skin_dataset'] / \"test\" / \"Healthy\",       # Skin - cani sani\n    paths['skin_dataset'] / \"test\" / \"Fungal_infections\",  # Skin - malati\n    paths['stanford_dogs'] / \"Images\" / \"n02085620-Chihuahua\",  # Stanford Dogs\n]\n\ntest_images = []\nfor test_dir in test_dirs:\n    if test_dir.exists():\n        images = list(test_dir.glob('*.jpg')) + list(test_dir.glob('*.png'))\n        test_images.extend(images[:2])  # Prendi 2 immagini per directory\n        print(f\"âœ“ {test_dir.name}: {len(images)} immagini trovate\")\n    else:\n        print(f\"âœ— {test_dir} non trovato\")\n\nprint(f\"\\nTotale: {len(test_images)} immagini di test\")\nfor img in test_images[:10]:\n    print(f\"  - {img.parent.name}/{img.name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Processa immagini di test\nif test_images:\n    for img_path in test_images[:4]:\n        print(f\"\\n{'='*50}\")\n        print(f\"Processing: {img_path.name}\")\n        print(f\"{'='*50}\")\n        \n        # Processa\n        detections = pipeline.process_image(str(img_path))\n        \n        print(f\"\\nRilevati {len(detections)} cani:\")\n        for i, det in enumerate(detections):\n            print(f\"\\nCane #{i+1}:\")\n            print(f\"  Confidence: {det.confidence:.2%}\")\n            print(f\"  Stray Index: {det.stray_index:.2%}\")\n            print(f\"  Is Stray: {'SÃŒ âš ï¸' if det.is_stray else 'NO âœ“'}\")\n            print(f\"  Components:\")\n            print(f\"    - P(no_collar): {det.p_no_collar:.2f}\")\n            print(f\"    - P(disease): {det.p_disease:.2f}\")\n            print(f\"    - P(stray_pose): {det.p_stray_pose:.2f}\")\n            print(f\"    - P(stray|{det.predicted_breed}): {det.p_stray_breed:.2f}\")\n        \n        # Visualizza - salva nella directory runs\n        output_dir = paths['runs_dir'] / 'demo'\n        output_dir.mkdir(parents=True, exist_ok=True)\n        visualize_detection(\n            str(img_path), \n            detections,\n            str(output_dir / f'demo_{img_path.stem}.png')\n        )\nelse:\n    print(\"Nessuna immagine di test trovata\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analisi SensibilitÃ  Pesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_weight_sensitivity(detection: DogDetection):\n    \"\"\"\n    Analizza come variano i risultati al variare dei pesi\n    \"\"\"\n    probs = [detection.p_no_collar, detection.p_disease, \n             detection.p_stray_pose, detection.p_stray_breed]\n    labels = ['Collar', 'Skin', 'Pose', 'Breed']\n    \n    # Varia ciascun peso\n    weight_range = np.linspace(0.1, 0.5, 20)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n    axes = axes.flatten()\n    \n    base_weights = [0.35, 0.20, 0.25, 0.20]\n    \n    for i, (ax, label) in enumerate(zip(axes, labels)):\n        stray_indices = []\n        \n        for w in weight_range:\n            # Modifica peso i e rinormalizza\n            weights = base_weights.copy()\n            weights[i] = w\n            total = sum(weights)\n            weights = [ww/total for ww in weights]\n            \n            stray_idx = sum(ww * p for ww, p in zip(weights, probs))\n            stray_indices.append(stray_idx)\n        \n        ax.plot(weight_range, stray_indices, 'b-', linewidth=2)\n        ax.axhline(y=0.6, color='r', linestyle='--', label='Threshold')\n        ax.axvline(x=base_weights[i], color='g', linestyle=':', \n                  label=f'Current ({base_weights[i]})')\n        ax.fill_between(weight_range, 0.6, stray_indices, \n                       where=[s >= 0.6 for s in stray_indices],\n                       alpha=0.3, color='red')\n        ax.set_xlabel(f'{label} Weight')\n        ax.set_ylabel('Stray Index')\n        ax.set_title(f'Sensitivity to {label} Weight')\n        ax.set_ylim(0, 1)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    \n    plt.suptitle(f'Weight Sensitivity Analysis\\nP=[{probs[0]:.2f}, {probs[1]:.2f}, {probs[2]:.2f}, {probs[3]:.2f}]')\n    plt.tight_layout()\n    \n    # Salva nella directory runs\n    output_dir = paths['runs_dir'] / 'demo'\n    output_dir.mkdir(parents=True, exist_ok=True)\n    plt.savefig(output_dir / 'weight_sensitivity.png', dpi=150)\n    plt.show()\n\n# Crea detection di esempio\nexample_detection = DogDetection(\n    bbox=(0, 0, 100, 100),\n    confidence=0.9,\n    p_no_collar=0.8,\n    p_disease=0.3,\n    p_stray_pose=0.6,\n    p_stray_breed=0.7,\n    stray_index=0.65,\n    is_stray=True\n)\n\nanalyze_weight_sensitivity(example_detection)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simulazione CCTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_cctv_frame(pipeline: ResQPetPipeline, image_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Simula l'elaborazione di un frame CCTV\n",
    "    Ritorna dati nel formato atteso dal frontend\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Processa\n",
    "    detections = pipeline.process_image(image_path)\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Formato per frontend\n",
    "    response = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'camera_id': 'CAM_001',\n",
    "        'processing_time_ms': processing_time * 1000,\n",
    "        'dogs_detected': len(detections),\n",
    "        'alerts': [],\n",
    "        'detections': []\n",
    "    }\n",
    "    \n",
    "    for det in detections:\n",
    "        response['detections'].append(det.to_dict())\n",
    "        \n",
    "        if det.is_stray:\n",
    "            alert = {\n",
    "                'type': 'STRAY_DETECTED',\n",
    "                'severity': 'high' if det.stray_index > 0.8 else 'medium',\n",
    "                'stray_index': det.stray_index,\n",
    "                'bbox': det.bbox,\n",
    "                'message': f\"Possibile cane randagio rilevato (Index: {det.stray_index:.0%})\"\n",
    "            }\n",
    "            response['alerts'].append(alert)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simula elaborazione CCTV\n",
    "if test_images:\n",
    "    print(\"=\"*50)\n",
    "    print(\"SIMULAZIONE CCTV\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for img_path in test_images[:3]:\n",
    "        result = simulate_cctv_frame(pipeline, str(img_path))\n",
    "        \n",
    "        print(f\"\\n--- Frame: {img_path.name} ---\")\n",
    "        print(f\"Camera: {result['camera_id']}\")\n",
    "        print(f\"Processing: {result['processing_time_ms']:.1f}ms\")\n",
    "        print(f\"Dogs detected: {result['dogs_detected']}\")\n",
    "        \n",
    "        if result['alerts']:\n",
    "            print(\"\\nâš ï¸ ALERTS:\")\n",
    "            for alert in result['alerts']:\n",
    "                print(f\"  [{alert['severity'].upper()}] {alert['message']}\")\n",
    "        else:\n",
    "            print(\"\\nâœ“ No alerts\")\n",
    "        \n",
    "        print(f\"\\nJSON Response:\")\n",
    "        print(json.dumps(result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Riepilogo e Prossimi Passi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RIEPILOGO PIPELINE ResQPet\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ“Š ARCHITETTURA:\")\n",
    "print(\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"  â”‚           YOLO11 Dog Pose (Backbone)        â”‚\")\n",
    "print(\"  â”‚        Detection + Keypoint Extraction      â”‚\")\n",
    "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(\"                        â”‚\")\n",
    "print(\"        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"        â–¼               â–¼               â–¼\")\n",
    "print(\"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"  â”‚ Collar   â”‚   â”‚  Skin    â”‚   â”‚  Pose    â”‚\")\n",
    "print(\"  â”‚ (YOLO)   â”‚   â”‚(ResNet50)â”‚   â”‚  (MLP)   â”‚\")\n",
    "print(\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(\"        â”‚               â”‚               â”‚\")\n",
    "print(\"        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(\"                        â–¼\")\n",
    "print(\"              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(\"              â”‚ Fusion (Weighted)â”‚\")\n",
    "print(\"              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "print(\"                        â”‚\")\n",
    "print(\"                        â–¼\")\n",
    "print(\"                  STRAY INDEX\")\n",
    "\n",
    "print(\"\\nâš–ï¸ PESI FUSION:\")\n",
    "print(f\"  â€¢ Collar:  {config.weight_collar:.0%} (indicatore principale)\")\n",
    "print(f\"  â€¢ Pose:    {config.weight_pose:.0%} (comportamento)\")\n",
    "print(f\"  â€¢ Skin:    {config.weight_skin:.0%} (condizioni)\")\n",
    "print(f\"  â€¢ Breed:   {config.weight_breed:.0%} (prior statistico)\")\n",
    "\n",
    "print(\"\\nğŸ“ MODELLI DISPONIBILI:\")\n",
    "models = [\n",
    "    (\"Backbone (YOLO11 Pose)\", config.backbone_model),\n",
    "    (\"Collar Detector\", config.collar_model),\n",
    "    (\"Skin Classifier\", config.skin_model),\n",
    "    (\"Pose Classifier\", config.pose_model),\n",
    "    (\"Breed Classifier\", config.breed_model)\n",
    "]\n",
    "\n",
    "for name, filename in models:\n",
    "    path = WEIGHTS_DIR / filename\n",
    "    status = \"âœ“\" if path.exists() else \"âš ï¸ (usando euristica)\"\n",
    "    print(f\"  {status} {name}: {filename}\")\n",
    "\n",
    "print(\"\\nğŸ”§ PROSSIMI PASSI:\")\n",
    "print(\"  1. Eseguire i notebook 00-04 per allenare i modelli\")\n",
    "print(\"  2. Avviare il backend: cd backend && python run.py\")\n",
    "print(\"  3. Avviare il frontend: cd frontend && npm run dev\")\n",
    "print(\"  4. Accedere a http://localhost:5173 per la GUI CCTV\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}