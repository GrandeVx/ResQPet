{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 03 - Training Stray Pose Classifier\n\nTraining del classificatore MLP per identificare posture \"stray-like\" dai keypoints.\n\n## IMPORTANTE: 24 Keypoints Dog-Pose\n\nQuesto notebook usa i **24 keypoints anatomici del cane** estratti dal modello `yolo11n-dog-pose.pt`.\n\n**PREREQUISITO**: Esegui prima:\n1. `00a_yolo_dog_pose_training.ipynb` - per addestrare il modello dog-pose\n2. `00_keypoints_extraction.ipynb` - per estrarre i keypoints dai dataset\n\n## Approccio Weak Supervision\n\nQuesto è il **contributo originale** del progetto:\n- I label NON sono annotati manualmente\n- Derivano dall'**origine del dataset**:\n  - FYP Dataset (cani randagi) → Label = 1 (Stray)\n  - Stanford Dogs / Skin Diseases (cani padronali) → Label = 0 (Owned)\n\n## Architettura\n- **Input**: 24 keypoints × 3 valori (x, y, visibility) = **72 features**\n- **Model**: MLP (72 → 128 → 64 → 1)\n- **Output**: P(stray_pose) ∈ [0, 1]\n\n## Keypoints del Cane (24)\n```\n 0: nose            8: withers           16: right_back_elbow\n 1: left_eye        9: left_front_elbow  17: left_back_knee\n 2: right_eye      10: right_front_elbow 18: right_back_knee\n 3: left_ear_base  11: left_front_knee   19: left_back_paw\n 4: right_ear_base 12: right_front_knee  20: right_back_paw\n 5: left_ear_tip   13: left_front_paw    21: tail_start\n 6: right_ear_tip  14: right_front_paw   22: tail_end\n 7: throat         15: left_back_elbow   23: chin\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installazione dipendenze\n",
    "%pip install torch numpy pandas scikit-learn matplotlib seaborn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve, accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configurazione paths - RELATIVI per portabilità\nimport sys\nsys.path.insert(0, str(Path.cwd()))\ntry:\n    from notebook_utils import get_paths, get_device, print_paths\n    paths = get_paths()\n    print_paths(paths)\nexcept ImportError:\n    print(\"notebook_utils.py non trovato, usando fallback...\")\n    NOTEBOOK_DIR = Path.cwd()\n    if NOTEBOOK_DIR.name == \"notebooks\":\n        PROJECT_DIR = NOTEBOOK_DIR.parent.parent\n    elif NOTEBOOK_DIR.name == \"training\":\n        PROJECT_DIR = NOTEBOOK_DIR.parent\n    else:\n        PROJECT_DIR = NOTEBOOK_DIR\n        while PROJECT_DIR.name != \"ResQPet\" and PROJECT_DIR.parent != PROJECT_DIR:\n            PROJECT_DIR = PROJECT_DIR.parent\n    BASE_DIR = PROJECT_DIR.parent\n    paths = {\n        'project_dir': PROJECT_DIR,\n        'base_dir': BASE_DIR,\n        'weights_dir': PROJECT_DIR / \"backend\" / \"weights\",\n        'data_dir': PROJECT_DIR / \"data\",\n    }\n    paths['weights_dir'].mkdir(parents=True, exist_ok=True)\n\n# Assegna variabili per retrocompatibilità\nBASE_DIR = paths['base_dir']\nDATA_DIR = paths['data_dir'] / \"keypoints\"\nOUTPUT_DIR = paths['weights_dir']\n\nprint(f\"\\nData dir: {DATA_DIR}\")\nprint(f\"Output dir: {OUTPUT_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Caricamento Dataset Keypoints\n",
    "\n",
    "Il dataset deve essere stato generato dal notebook `00_keypoints_extraction.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Carica dataset\ndataset_path = DATA_DIR / 'pose_keypoints_dataset.csv'\n\nif dataset_path.exists():\n    df = pd.read_csv(dataset_path)\n    print(f\"Dataset caricato: {len(df)} samples\")\n    print(f\"Colonne: {len(df.columns)}\")\n    print(f\"\\nDistribuzione label:\")\n    print(df['label'].value_counts())\nelse:\n    print(f\"Dataset non trovato: {dataset_path}\")\n    print(\"\\nEsegui prima il notebook 00_keypoints_extraction.ipynb\")\n    print(\"\\nPer ora, creiamo un dataset sintetico per demo...\")\n    \n    # Crea dataset sintetico per demo\n    # IMPORTANTE: 24 keypoints come da documentazione (dog-pose, NON human-pose)\n    np.random.seed(42)\n    n_samples = 2000\n    n_keypoints = 24  # 24 keypoints anatomici del cane (NON 17 di human-pose!)\n    \n    # Simula keypoints per cani stray (posture più rannicchiate)\n    stray_kpts = np.random.randn(n_samples // 2, n_keypoints * 3) * 0.3\n    stray_kpts[:, 1::3] += 0.2  # y più alto (testa bassa)\n    stray_labels = np.ones(n_samples // 2)\n    \n    # Simula keypoints per cani owned (posture più aperte)\n    owned_kpts = np.random.randn(n_samples // 2, n_keypoints * 3) * 0.3\n    owned_kpts[:, 0::3] += 0.1  # x più largo\n    owned_labels = np.zeros(n_samples // 2)\n    \n    # Combina\n    X_synthetic = np.vstack([stray_kpts, owned_kpts])\n    y_synthetic = np.concatenate([stray_labels, owned_labels])\n    \n    # Crea DataFrame con nomi keypoints corretti\n    columns = ['label'] + [f'kpt_{i}_{c}' for i in range(n_keypoints) for c in ['x', 'y', 'v']]\n    df = pd.DataFrame(\n        np.column_stack([y_synthetic, X_synthetic]),\n        columns=columns\n    )\n    \n    print(f\"Dataset sintetico creato: {len(df)} samples\")\n    print(f\"Keypoints: {n_keypoints} (24 anatomici del cane)\")\n    print(f\"Features: {n_keypoints * 3} (72 = 24 × 3)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza prime righe\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparazione Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrai features (colonne keypoints)\n",
    "kpt_columns = [c for c in df.columns if c.startswith('kpt_')]\n",
    "print(f\"Keypoint columns: {len(kpt_columns)}\")\n",
    "\n",
    "# Features e labels\n",
    "X = df[kpt_columns].values.astype(np.float32)\n",
    "y = df['label'].values.astype(np.float32)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y.astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestisci valori mancanti\n",
    "print(f\"NaN values: {np.isnan(X).sum()}\")\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "# Split train/val/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizzazione\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train mean: {X_train_scaled.mean():.4f}, std: {X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_scaled),\n",
    "    torch.FloatTensor(y_train)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val_scaled),\n",
    "    torch.FloatTensor(y_val)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_test_scaled),\n",
    "    torch.FloatTensor(y_test)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modello MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class StrayPoseMLP(nn.Module):\n    \"\"\"\n    MLP per classificazione postura stray/owned.\n    \n    Architettura:\n    - Input: keypoints flattened (24 keypoints × 3 = 72 features per dog-pose)\n    - Hidden: 128 → 64 con ReLU, BatchNorm, Dropout\n    - Output: 1 (sigmoid per probabilità)\n    \n    NOTA: L'ordine dei layer è Linear → ReLU → BatchNorm → Dropout\n    per corrispondere al modello nel backend.\n    \"\"\"\n    \n    def __init__(self, input_dim=72, hidden_dims=[128, 64], dropout=0.3):\n        super().__init__()\n        \n        layers = []\n        prev_dim = input_dim\n        \n        for hidden_dim in hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n                nn.BatchNorm1d(hidden_dim),\n                nn.Dropout(dropout)\n            ])\n            prev_dim = hidden_dim\n        \n        layers.append(nn.Linear(prev_dim, 1))\n        layers.append(nn.Sigmoid())\n        \n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x).squeeze(-1)\n\n\n# Crea modello\n# INPUT_DIM viene calcolato dal dataset (dovrebbe essere 72 per 24 keypoints × 3)\nINPUT_DIM = X_train_scaled.shape[1]\n\n# Verifica che sia 72 (24 keypoints × 3)\nexpected_dim = 24 * 3  # 72\nif INPUT_DIM != expected_dim:\n    print(f\"⚠️ ATTENZIONE: INPUT_DIM = {INPUT_DIM}, ma ci si aspetta {expected_dim} (24 keypoints × 3)\")\n    print(f\"   Probabilmente il dataset è stato estratto con il modello sbagliato.\")\n    print(f\"   Ri-esegui 00_keypoints_extraction.ipynb con yolo11n-dog-pose.pt\")\nelse:\n    print(f\"✓ INPUT_DIM = {INPUT_DIM} (24 keypoints × 3) - Corretto!\")\n\nmodel = StrayPoseMLP(input_dim=INPUT_DIM, hidden_dims=[128, 64], dropout=0.3)\nmodel = model.to(DEVICE)\n\nprint(f\"\\nModel:\")\nprint(model)\nprint(f\"\\nParameters: {sum(p.numel() for p in model.parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 15\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for X_batch, y_batch in loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(outputs.detach().cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    # Calcola metriche\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    acc = accuracy_score(all_labels, (all_preds > 0.5).astype(int))\n",
    "    \n",
    "    return total_loss / len(loader), acc, auc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    acc = accuracy_score(all_labels, (all_preds > 0.5).astype(int))\n",
    "    \n",
    "    return total_loss / len(loader), acc, auc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"=\"*50)\n",
    "print(\"INIZIO TRAINING STRAY POSE CLASSIFIER\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nApproccio: WEAK SUPERVISION\")\n",
    "print(f\"  - Stray labels: da FYP Dataset\")\n",
    "print(f\"  - Owned labels: da Stanford Dogs / Skin Diseases\")\n",
    "print()\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_auc': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_auc': []\n",
    "}\n",
    "\n",
    "best_auc = 0\n",
    "patience_counter = 0\n",
    "best_model_path = OUTPUT_DIR / 'stray_pose_classifier_best.pt'\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc, train_auc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_auc, _, _ = evaluate(\n",
    "        model, val_loader, criterion, DEVICE\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_auc)\n",
    "    \n",
    "    # Log\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_auc'].append(train_auc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_auc'].append(val_auc)\n",
    "    \n",
    "    # Print progress ogni 10 epoche\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f}\")\n",
    "        print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_auc': val_auc,\n",
    "            'val_acc': val_acc,\n",
    "            'input_dim': INPUT_DIM,\n",
    "            'scaler_mean': scaler.mean_,\n",
    "            'scaler_scale': scaler.scale_\n",
    "        }, best_model_path)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  ✓ Best model saved (AUC: {val_auc:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETATO!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Valutazione Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Carica best model\n# NOTA: weights_only=False necessario per caricare scaler numpy\ncheckpoint = torch.load(best_model_path, weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"Best model loaded from epoch {checkpoint['epoch']+1}\")\nprint(f\"Val AUC: {checkpoint['val_auc']:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test finale\n",
    "test_loss, test_acc, test_auc, test_preds, test_labels = evaluate(\n",
    "    model, test_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  AUC-ROC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "test_preds_binary = (test_preds > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_preds_binary,\n",
    "                           target_names=['Owned', 'Stray']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazioni\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_preds)\n",
    "axes[0, 0].plot(fpr, tpr, label=f'AUC = {test_auc:.3f}')\n",
    "axes[0, 0].plot([0, 1], [0, 1], 'k--')\n",
    "axes[0, 0].set_xlabel('False Positive Rate')\n",
    "axes[0, 0].set_ylabel('True Positive Rate')\n",
    "axes[0, 0].set_title('ROC Curve')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds_binary)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1],\n",
    "           xticklabels=['Owned', 'Stray'], yticklabels=['Owned', 'Stray'])\n",
    "axes[0, 1].set_xlabel('Predicted')\n",
    "axes[0, 1].set_ylabel('True')\n",
    "axes[0, 1].set_title('Confusion Matrix')\n",
    "\n",
    "# 3. Training curves - Loss\n",
    "axes[1, 0].plot(history['train_loss'], label='Train')\n",
    "axes[1, 0].plot(history['val_loss'], label='Val')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title('Training Loss')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Training curves - AUC\n",
    "axes[1, 1].plot(history['train_auc'], label='Train')\n",
    "axes[1, 1].plot(history['val_auc'], label='Val')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('AUC')\n",
    "axes[1, 1].set_title('AUC-ROC')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR.parent.parent / 'training' / 'notebooks' / 'pose_training_results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuzione predizioni\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Per classe\n",
    "for label, name in [(0, 'Owned'), (1, 'Stray')]:\n",
    "    mask = test_labels == label\n",
    "    axes[0].hist(test_preds[mask], bins=30, alpha=0.6, label=name)\n",
    "axes[0].axvline(x=0.5, color='r', linestyle='--', label='Threshold')\n",
    "axes[0].set_xlabel('P(stray_pose)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribuzione Predizioni per Classe')\n",
    "axes[0].legend()\n",
    "\n",
    "# Overall\n",
    "axes[1].hist(test_preds, bins=30, edgecolor='black')\n",
    "axes[1].axvline(x=0.5, color='r', linestyle='--', label='Threshold')\n",
    "axes[1].set_xlabel('P(stray_pose)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Distribuzione Complessiva Predizioni')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR.parent.parent / 'training' / 'notebooks' / 'pose_prediction_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva modello finale\n",
    "final_model_path = OUTPUT_DIR / 'stray_pose_classifier.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'input_dim': INPUT_DIM,\n",
    "    'hidden_dims': [128, 64],\n",
    "    'scaler_mean': scaler.mean_,\n",
    "    'scaler_scale': scaler.scale_,\n",
    "    'test_auc': test_auc,\n",
    "    'test_acc': test_acc,\n",
    "    'training_approach': 'weak_supervision'\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"Modello salvato in: {final_model_path}\")\n",
    "print(f\"Dimensione: {final_model_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test modello esportato\nprint(\"\\nTest modello esportato...\")\n\n# Carica (weights_only=False per numpy arrays)\nloaded = torch.load(final_model_path, weights_only=False)\ntest_model = StrayPoseMLP(\n    input_dim=loaded['input_dim'],\n    hidden_dims=loaded['hidden_dims']\n)\ntest_model.load_state_dict(loaded['model_state_dict'])\ntest_model.eval()\n\n# Test su un campione\nsample = X_test_scaled[0:1]\nsample_tensor = torch.FloatTensor(sample)\n\nwith torch.no_grad():\n    pred = test_model(sample_tensor).item()\n\nprint(f\"\\nSample prediction:\")\nprint(f\"  P(stray_pose): {pred:.4f}\")\nprint(f\"  True label: {'Stray' if y_test[0] == 1 else 'Owned'}\")\nprint(f\"  Predicted: {'Stray' if pred > 0.5 else 'Owned'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riepilogo finale\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RIEPILOGO TRAINING STRAY POSE CLASSIFIER\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nApproccio: WEAK SUPERVISION (Contributo Originale)\")\n",
    "print(f\"  - Labels derivati dall'origine del dataset\")\n",
    "print(f\"  - Nessuna annotazione manuale richiesta\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  - Train: {len(X_train)}\")\n",
    "print(f\"  - Val: {len(X_val)}\")\n",
    "print(f\"  - Test: {len(X_test)}\")\n",
    "print(f\"  - Features: {INPUT_DIM}\")\n",
    "print(f\"\\nArchitettura: MLP ({INPUT_DIM} → 128 → 64 → 1)\")\n",
    "print(f\"\\nRisultati Test:\")\n",
    "print(f\"  - AUC-ROC: {test_auc:.4f}\")\n",
    "print(f\"  - Accuracy: {test_acc:.4f}\")\n",
    "print(f\"\\nModello salvato: {final_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}